import os
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, ServiceContext
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI

# === CONFIGURA√á√ÉO ===
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TXT_FILE = "transcricoes.txt"
OUTPUT_DIR = "storage"

# === VERIFICA A CHAVE ===
if not OPENAI_API_KEY:
    raise ValueError("A vari√°vel de ambiente OPENAI_API_KEY n√£o est√° definida.")

# === PREPARAR DOCUMENTO ===
if not os.path.exists(TXT_FILE):
    raise FileNotFoundError(f"O arquivo {TXT_FILE} n√£o foi encontrado.")

# Copia o txt para uma pasta tempor√°ria
os.makedirs("tmp", exist_ok=True)
os.system(f"copy {TXT_FILE} tmp\\conteudo.txt" if os.name == "nt" else f"cp {TXT_FILE} tmp/conteudo.txt")

# === CRIAR √çNDICE ===
print("üìÑ Lendo conte√∫do...")
documents = SimpleDirectoryReader("tmp").load_data()

print("üß† Gerando embeddings com OpenAI...")
embed_model = OpenAIEmbedding(model="text-embedding-3-small", api_key=OPENAI_API_KEY)
llm = OpenAI(model="gpt-3.5-turbo", api_key=OPENAI_API_KEY)
service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm)

print("üîç Criando √≠ndice vetorial...")
index = VectorStoreIndex.from_documents(documents, service_context=service_context)

print("üíæ Salvando √≠ndice em:", OUTPUT_DIR)
index.storage_context.persist(persist_dir=OUTPUT_DIR)

print("‚úÖ √çndice gerado com sucesso!")
