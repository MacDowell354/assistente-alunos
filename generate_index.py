import os
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.core.node_parser import SentenceSplitter
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core import Settings
from llama_index.core.schema import Document
from openai import OpenAI

# ==== CONFIG ====
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TRANSCRIPTION_FILE = "transcricoes.txt"
OUTPUT_DIR = "storage"

# ==== VERIFY KEY ====
if not OPENAI_API_KEY:
    raise ValueError("A vari√°vel de ambiente OPENAI_API_KEY n√£o est√° definida.")

# ==== PREPARE DOCUMENT ====
if not os.path.exists(TRANSCRIPTION_FILE):
    raise FileNotFoundError(f"O arquivo {TRANSCRIPTION_FILE} n√£o foi encontrado.")

# Copia o .txt para uma pasta tempor√°ria
os.makedirs("tmp", exist_ok=True)
os.system(f"cp {TRANSCRIPTION_FILE} tmp/conteudo.txt")

# ==== CRIAR √çNDICE ====
print("üìÑ Lendo conte√∫do do arquivo...")
with open(f"tmp/conteudo.txt", "r", encoding="utf-8") as f:
    full_text = f.read()

# ==== DIVIDIR EM CHUNKS ====
print("‚úÇÔ∏è Dividindo em trechos...")
parser = SentenceSplitter(chunk_size=512, chunk_overlap=50)
document = Document(text=full_text)
nodes = parser.get_nodes_from_documents([document])

# ==== EMBEDDINGS ====
print("üß† Gerando embeddings com OpenAI...")
Settings.embed_model = OpenAIEmbedding(
    model="text-embedding-3-small",
    api_key=OPENAI_API_KEY,
)

# ==== CRIAR DOCUMENTOS E √çNDICE ====
index = VectorStoreIndex.from_documents(nodes)

# ==== SALVAR O √çNDICE ====
print(f"üíæ Salvando √≠ndice em: {OUTPUT_DIR}")
index.storage_context.persist(persist_dir=OUTPUT_DIR)

print("‚úÖ √çndice gerado com sucesso!")
