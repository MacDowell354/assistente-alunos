import os

# Leitor de arquivos locais
from llama_index.readers.simple_directory_reader import SimpleDirectoryReader
# Cria√ß√£o de √≠ndice vetorial
from llama_index.indices.vector_store import GPTVectorStoreIndex
# Persist√™ncia do √≠ndice
from llama_index.storage.storage_context import StorageContext
# Embeddings OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
# Contexto de servi√ßo (combina LLM + Embedding)
from llama_index.service_context import ServiceContext

# --- Configura√ß√µes ---
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
INDEX_DIR = "storage"

# Cria o ServiceContext com o embedding da OpenAI
service_context = ServiceContext.from_defaults(
    embed_model=OpenAIEmbedding(
        model="text-embedding-3-small",
        api_key=OPENAI_API_KEY
    )
)

# Garante que ./storage exista
os.makedirs(INDEX_DIR, exist_ok=True)

# Se estiver vazio, gera o √≠ndice; sen√£o, apenas informa
if not os.listdir(INDEX_DIR):
    print("üóÇÔ∏è Gerando √≠ndice a partir de transcricoes.txt‚Ä¶")
    docs = SimpleDirectoryReader(input_files=["transcricoes.txt"]).load_data()
    index = GPTVectorStoreIndex.from_documents(docs, service_context=service_context)

    storage_ctx = StorageContext.from_defaults(persist_dir=INDEX_DIR)
    index.storage_context = storage_ctx
    storage_ctx.persist(persist_dir=INDEX_DIR)
    print(f"‚úÖ √çndice gerado em '{INDEX_DIR}'")
else:
    print(f"‚ÑπÔ∏è  √çndice j√° existe em '{INDEX_DIR}'")
